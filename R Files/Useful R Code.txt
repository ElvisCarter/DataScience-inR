#Some, hopefully usefull, R-code for Project1 COSC 4335 Spring 2018
#Author: Christoph F. Eick
#last updated January 20, 2018


#Code for Scatter Plots
setwd("C:\\Users\\ceick\\Desktop\\UDM")
#Load Pima Indian Diabetes Dataset, 
#next creating a data frame renaming attributes to a-h and class attribute as z
a<-read.csv("pid.data")
d<-data.frame(a=a[,1],b=a[,2],c=a[,3],d=a[,4],e=a[,5],f=a[,6],g=a[,7],h=a[,8],z=factor(a[,9]))
plot(d$a,d$b)
plot(d$c,d$d)
require("lattice")
plot(d$a, d$c, pch=21, 
bg=c("red","blue")[unclass(d$z)], 
main="PID Scatterplot attributes 1+3")
plot(d$b, d$g, pch=21, 
bg=c("red","blue")[unclass(d$z)], 
main="PID Scatterplot attributes 2+7")
plot(d$f, d$g, pch=21, 
bg=c("red","blue")[unclass(d$z)], 
main="PID Scatterplot attributes 6+7")
plot(d$b, d$f, pch=21, 
bg=c("red","blue")[unclass(d$z)], 
main="PID Scatterplot attributes 2+6")
xyplot(f ~ h | z, d, groups = z, pch= 20)
require("ggplot2")
ggplot(d, aes(x = b, y = f, colour = z)) + geom_point()
ggplot(d, aes(x = b, y = h, colour = z)) + geom_point()
ggplot(d, aes(x = g, y = h, colour = z)) + geom_point()
ggplot(d, aes(x = g, y = h)) + geom_point() + facet_grid(~z)
ggplot (d, aes (x = b, y = g, colour = z)) + stat_density2d ()
ggplot (d, aes (x = b, y = f, colour = z)) + stat_density2d ()
ggplot (d, aes (x = g, y = h, colour = z)) + stat_density2d ()
require("hexbin")
ggplot (d, aes (x = b, y = f, fill = z)) + stat_binhex (bins=5, aes (alpha = ..count..)) + 
facet_grid (. ~ z)

#Z-scoring the Banknote Authentication Dataset
#The dataset can be found at: https://archive.ics.uci.edu/ml/datasets/banknote+authentication#
setwd("C:\\Users\\ceick\\Desktop\\UDM")
a<-read.csv("bna.csv")
sbna<-data.frame(a=a[,1],b=a[,2],c=a[,3],d=a[,4])
s<-scale(sbna)
#This function z-scores the 4 attributes of the bna dataset
nbna<-data.frame(a=s[,1],b=s[,2],c=s[,3],d=s[,4], e=factor(a[,5]), z=a[,5])
print(nbna[1:10,])
print(mean(nbna[,1]))
print(mean(nbna[,2]))
print(sd(nbna[,1]))
print(sd(nbna[,2]))
#as we see from the output, all attributes of the dataset nbna have a mean value of 0 and standard deviation of 1
#Next, we fit a linear model to the z-scored dataset that predicts the two classes (0 or 1)
mod<-lm(z ~ a + b + c + d, data=nbna)
mod$coeff
summary(mod)$r.squared
#As the results we obtain the following linear function:
#      (Intercept)            a            b            c            d 
#         0.444930708 -0.405319268 -0.459650306 -0.437994319 -0.001767647 
#      that is z= 0.44 + -0.41*a + -0.46*b + -0.44*c
# we infer that attibute d is unimportant, and the other three attributes are equally important; 
# moreover, if attributes a, b, c have low values---which means their z-scored values are negative--- class 1 
# is more likely, as they all have negative coefficients; finally, the R_Squared is 
# suprisingly high as it is 0.86 which means the linear model fits the data quite well!
ggplot (nbna, aes (x = a, y = b, fill = e)) + stat_binhex (bins=5, aes (alpha = ..count..)) + 
facet_grid (. ~ e)
ggplot (nbna, aes (x = a, y = c, fill = e)) + stat_binhex (bins=5, aes (alpha = ..count..)) + 
facet_grid (. ~ e)
#Particularly, the second display reveals that the classes are quite separated in the a-c-space
xyplot(a ~ c | e, nbna, groups = e, pch= 20)

#Getting some decision trees for the Pima Indian Diabetes Dataset
setwd("C:\\Users\\ceick\\Desktop\\UDM")
a<-read.csv("pid.data")
d<-data.frame(a=a[,1],b=a[,2],c=a[,3],d=a[,4],e=a[,5],f=a[,6],g=a[,7],h=a[,8],z=factor(a[,9]))
require("rpart")
model<-rpart(z ~ a + b + c + d + e + f + g + h,
	data=d,
	method="class", maxdepth=5)
plot(model, uniform=TRUE, branch=0.2, margin=0.2)
text(model,use.n=TRUE,all=TRUE,cex=0.8)

model<-rpart(z ~ a + b + c + d + e + f + g + h,
	data=d,
	method="class",
        control=rpart.control(cp=0.01, maxdepth=4))
plot(model, margin=0.1)
text(model,use.n=TRUE,all=TRUE,cex=0.8)


model<-rpart(z ~ a + b + c + d + e + f + g + h,
	data=d,
	method="class",
        control=rpart.control(cp=0.0006, maxdepth=4))
plot(model, compress=TRUE, uniform=TRUE, margin=0.15)
text(model,use.n=TRUE,all=TRUE,cex=1.1)

model<-rpart(z ~ a + b + c + d + e + f + g + h,
	data=d,
	method="class",
        control=rpart.control(cp=0.0002, maxdepth=5))
plot(model, compress=TRUE, uniform=TRUE, margin=0.12)
text(model,use.n=TRUE,all=TRUE,cex=1.0)

require("party")
model2<-ctree(z ~ a + b + c + d + e + f + g + h,
	data=d,
        controls=ctree_control(maxdepth=3))
plot(model2)
model3<-ctree(z ~ a + b + c + d + e + f + g + h,
	data=d,
        controls=ctree_control(maxdepth=7))
plot(model3)



rm(list=ls(all=TRUE))

setwd("<path-to>/RExamples")
complex9 <- read.csv("complex9.txt", header=FALSE)
ff <- read.csv("forestfires.csv", header=TRUE)

library(cluster)

# As your reference:
#   - a function to transform data.frame
#   - functions to calculate entropy and variance, as per assignment-2
# Try to:
#   - rewrite the functions in imperative style (no map/apply/reduce)
#   - rewrite the functions in full functional style (no for-loop)
# Conclusion: balance your self between R-idioms and good-old-familiar loops

transformff <- function(ff) {
  ff[,1] <- (ff[,1] - 1) / 8
  ff[,2] <- (ff[,2] - 2) / 7

  ff[,3] <- sapply(ff[,3], function(x) {
    if (x %in% c("dec", "jan", "feb", "mar")) return (0)
    if (x %in% c("jun", "jul", "aug", "sep")) return (1)
    return (1/2)
  })

  ff[,4] <- sapply(ff[,4], function(x) {
    if (x %in% c("sat", "sun")) return (0)
    if (x %in% c("fri", "mon")) return (1/2)
    return (1)
  })

  ff[,5:12] <- apply(ff[,5:12], 2, function(x) {
    m <- mean(x)
    z <- sd(x)
    return ((x - m) / z)
  })

  ff[,13] <- log10(ff[,13] + 1)

  ff$a14 <- sapply(ff[,13], function(x) {
    if (x == 0) return ('N')
    if (x < 1 ) return ('M')
    return ('H')
  })

  return (ff)
}

transff <- transformff(ff)

entropyvec <- function(vec) {
  s <- sum(vec)

  totalh <- Reduce(function(acc, elem) {
    prob <- elem / s
    h <- if (prob == 0) 0 else prob * log2(prob)
    return (acc - h)
  }, vec, 0)

  return (totalh)
}

entropy <- function(clusterassignment, groundtruth) {
  clusterlevels <- levels(clusterassignment)
  clusterclass <- table(clusterassignment, groundtruth)

  n <- nrow(clusterclass)
  entropy_weight <- array(dim=c(n, 2))
  population <- 0

  for (i in 1:n) {
    clustersize <- sum(clusterclass[i,])
    population <- population + clustersize
    entropy_weight[i,] = c(entropyvec(clusterclass[i,]), clustersize)
  }

  percentage <- 0
  if (clusterlevels[1] == "0") {
    outliers <- sum(clusterclass[1,])
    percentage <- outliers / population
    population <- population - outliers
    entropy_weight[1,] = c(0, 0)
  }

  totalh <- 0
  if (population > 0) {
    for (i in 1:n) {
      totalh <- totalh + entropy_weight[i, 1] * entropy_weight[i, 2] / population
    }
  }

  return (c(totalh, percentage))
}

variance <- function(clusterassignment, numerical) {
  clusterlevels <- levels(clusterassignment)
  n <- length(clusterlevels)
  var_weight <- array(dim=c(n, 2))
  population <- 0
  b <- 1
  outliers <- 0

  if (clusterlevels[1] == "0") {
    b <- 2
    var_weight[1,] <- c(0, 0)
    outliers <- length(numerical[clusterassignment == "0"])
  }

  for (i in b:n) {
    cluster <- numerical[clusterassignment==clusterlevels[i]]
    population <- population + length(cluster)
    if (length(cluster) == 1) {
      var_weight[i,] <- c(0, length(cluster))
    } else {
      var_weight[i,] <- c(var(cluster), length(cluster))
    }
  }

  percentage <- outliers / (outliers + population)

  totalv <- 0
  for (i in 1:n) {
    totalv <- totalv + var_weight[i, 1] * var_weight[i, 2] / population
  }
  return (c(totalv, percentage))
}

km1 <- kmeans(complex9[1:2], 4)
res1 <- data.frame(complex9, cluster=factor(km1$cluster))
entropy(res1$cluster, complex9$V3)

km2 <- kmeans(transff[1:12], 5, nstart=20)
entropy(factor(km2$cluster), transff[,14])
variance(factor(km2$cluster), transff[,13])
table(km2$cluster, transff[,14])


help(apply)
?(apply)

setwd("<path-to>/RExamples")

# No scalar, vector is the defaul data structure
# Homegeneous
vec1 <- c(1, 2, 3, 4)
is.vector(vec1)
is.list(vec1)
str(vec1)
typeof(vec1)

# Heterogenous
l1 <- list(1, 2, 3)

# Vectors is flat. List is recursive
vec2 <- c(1, c(2, c(3, 4)))
l2 <- list(1, list(2, list(3)))

# for loop using index
for (i in 1: length(vec1)) {
  print(vec1[i])
}

# for loop using iterator
for (x in vec1) {
  print(x)
}

# while loop (rarely used)
counter <- 0
while (counter < 10) {
  counter <- counter + 1
  if (counter %% 2 == 0)
    next
  print(counter)
}

print((1:10)[(1:10) %% 2 == 0])

# repeat loop (almost never used)
# break: the only way to stop the loop
counter <- 0
repeat {
  print(counter)
  counter <- counter + 1
  if (counter == 5) break
}

total <- 0
for (x in vec1) {
  total <- total + x
}

# Functional style: mapping, reducing

# For vectors
# Most operations/functions are "mapped"
vec3 <- vec1 * 2

# others are "reduced" (aggregated)
sum(vec1)
mean(vec1)


foo <- function(x) {
  return (x + 1)
}

foo(vec1)

# These are not applicable for list
foo(l1)
sum(l1)

for (x in l1) { print(foo(x)) }

# construct list, note the assignment
l3 <- list()
for (x in l1) { l3 <- append(l3, x) }

# Map always returns list
res <- Map(foo, l1)
typeof(res)

res <- Map(foo, vec1)
typeof(res)

# Reduce
bar <- function(x, y) { return (x + y) }
Reduce(bar, l1)

# Our version of mean() that can operate on "iterable" data
myMean <- function(x) {
  return (Reduce(bar, x) / length(x)
}

# array, matrix (array' special case)
# similar to vector, i.e. "flat"
m <- matrix(0, nrow = 2, ncol = 3)
m[,] <- runif(2*3, 0, 1)

# Not what we want
Map(function(x) {x^2}, m)

# apply on matrix
apply(m, 1, function(x) x^2)

# Can we use Map? - Yes
# (Also using anonymous function)
nrow(m)
ncol(m)
res <- matrix(0, nrow=nrow(m), ncol=ncol(m))
for (i in 1:nrow(m)) {
  res[i,] <- unlist(Map(function(x) x^2, m[i,]))
}

# list-apply
lapply(1:3, function(x) x^2)

# simplified-apply
sapply(1:3, function(x) x^2)

# Becareful with side-effect
# Compare the following...
mapply(function(x) print(x), l1)
mappedAction <- mapply(function(x) print(x), l1)


#Author: Christoph Eick; February 9, 2018.
#Dealing with NA's
a<-c(1,NA,4,7)
sd(a)
sd(a, na.rm=TRUE)
mean(a, na.rm=TRUE)
cor(iris[1:4])
a<-c(NA,NA,5,6,7,8, 9, 10, 12)
b<-c(1,2,NA,NA,4,4, 7, 2, 4)
c<-c(3,3,1,6,NA,NA, 5, -4, -6)
x<-data.frame(a,b,c)
x
cor(x) 
cor(x, use="complete.obs")
cor(x, use="pairwise.complete.obs")
cov(x, use="pairwise.complete.obs")
#also see: http://thomasleeper.com/Rcourse/Tutorials/NAhandling.html
#I recommend using "pairwise.complete.obs", but this can also lead to problems: http://bwlewis.github.io/covar/missing.html